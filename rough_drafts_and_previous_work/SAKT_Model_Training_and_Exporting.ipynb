{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:16:46.545565Z",
          "iopub.status.busy": "2020-10-31T03:16:46.544967Z",
          "iopub.status.idle": "2020-10-31T03:16:48.078462Z",
          "shell.execute_reply": "2020-10-31T03:16:48.077886Z"
        },
        "papermill": {
          "duration": 1.552332,
          "end_time": "2020-10-31T03:16:48.078580",
          "exception": false,
          "start_time": "2020-10-31T03:16:46.526248",
          "status": "completed"
        },
        "tags": [],
        "id": "4ge-Xz7FkxGE"
      },
      "outputs": [],
      "source": [
        "#JEREMY removed above cell since only bottom code is needed to operate in Google Colab (we are not using Kaggle)\n",
        "#utils.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd #HDKIM\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "def make_std_mask(x, pad):\n",
        "    \"Create a mask to hide padding and future words.\"\n",
        "    mask = torch.unsqueeze((x!=pad), -1)\n",
        "\n",
        "    tgt_mask = mask & Variable(\n",
        "        subsequent_mask(x.size(-1)).type_as(mask.data))\n",
        "    #         print('tgt_mask size after: ', tgt_mask.size())\n",
        "    return tgt_mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#JEREMY adding non-cuda functionality\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RbhgMm-OqynE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:16:48.108900Z",
          "iopub.status.busy": "2020-10-31T03:16:48.108099Z",
          "iopub.status.idle": "2020-10-31T03:16:49.425613Z",
          "shell.execute_reply": "2020-10-31T03:16:49.425028Z"
        },
        "papermill": {
          "duration": 1.33471,
          "end_time": "2020-10-31T03:16:49.425720",
          "exception": false,
          "start_time": "2020-10-31T03:16:48.091010",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dha1b8Z0kxGE",
        "outputId": "81fc102b-e171-423b-f0bd-6930060770bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2025 NVIDIA Corporation\n",
            "Built on Fri_Feb_21_20:23:50_PST_2025\n",
            "Cuda compilation tools, release 12.8, V12.8.93\n",
            "Build cuda_12.8.r12.8/compiler.35583870_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!rm -rf ~/.nv/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:16:49.475242Z",
          "iopub.status.busy": "2020-10-31T03:16:49.473330Z",
          "iopub.status.idle": "2020-10-31T03:16:49.475899Z",
          "shell.execute_reply": "2020-10-31T03:16:49.476390Z"
        },
        "papermill": {
          "duration": 0.038171,
          "end_time": "2020-10-31T03:16:49.476506",
          "exception": false,
          "start_time": "2020-10-31T03:16:49.438335",
          "status": "completed"
        },
        "tags": [],
        "id": "ccwEToS8kxGE"
      },
      "outputs": [],
      "source": [
        "# multihead_attn.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.nn import LayerNorm\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "def attention(query, key, value, key_masks=None, query_masks=None, future_masks=None, dropout=None, infer=False):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    layernorm = LayerNorm(d_k).to(device)\n",
        "    # query shape = [nbatches, h, T_q, d_k]       key shape = [nbatches, h, T_k, d_k] == value shape\n",
        "    # scores shape = [nbatches, h, T_q, T_k]  == p_attn shape\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    # if key_masks is not None:\n",
        "    #     scores = scores.masked_fill(key_masks.unsqueeze(1).cuda() == 0, -1e9)\n",
        "    if future_masks is not None:\n",
        "        scores = scores.masked_fill(future_masks.unsqueeze(0).to(device) == 0, -1e9)\n",
        "\n",
        "\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    outputs = p_attn\n",
        "    # if query_masks is not None:\n",
        "    #     outputs = outputs * query_masks.unsqueeze(1)\n",
        "    if dropout is not None:\n",
        "        outputs = dropout(outputs)\n",
        "    outputs = torch.matmul(outputs, value)\n",
        "\n",
        "    outputs += query\n",
        "    return layernorm(outputs), p_attn\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.2, infer=False):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layernorm = LayerNorm(d_model).to(device)\n",
        "        self.infer = infer\n",
        "\n",
        "    def forward(self, query, key, value, key_masks=None, query_masks=None, future_masks=None):\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = \\\n",
        "            [F.relu(l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2), inplace=True)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        # k v shape = [nbatches, h, T_k, d_k],  d_k * h = d_model\n",
        "        # q shape = [nbatches, h, T_q, d_k]\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(query, key, value, query_masks=query_masks,\n",
        "                                 key_masks=key_masks, future_masks=future_masks, dropout=self.dropout, infer=self.infer)\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:16:49.506981Z",
          "iopub.status.busy": "2020-10-31T03:16:49.506182Z",
          "iopub.status.idle": "2020-10-31T03:17:18.642386Z",
          "shell.execute_reply": "2020-10-31T03:17:18.641845Z"
        },
        "papermill": {
          "duration": 29.152711,
          "end_time": "2020-10-31T03:17:18.642509",
          "exception": false,
          "start_time": "2020-10-31T03:16:49.489798",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG3XM60EkxGF",
        "outputId": "2982f775-bf23-4e49-ecd7-a102e3784751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Requirement '../input/humanfriendly82/humanfriendly-8.2-py2.py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
            "\u001b[0mProcessing /input/humanfriendly82/humanfriendly-8.2-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/input/humanfriendly82/humanfriendly-8.2-py2.py3-none-any.whl'\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install ../input/humanfriendly82/humanfriendly-8.2-py2.py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:17:18.675632Z",
          "iopub.status.busy": "2020-10-31T03:17:18.674752Z",
          "iopub.status.idle": "2020-10-31T03:17:46.235642Z",
          "shell.execute_reply": "2020-10-31T03:17:46.234998Z"
        },
        "papermill": {
          "duration": 27.579558,
          "end_time": "2020-10-31T03:17:46.235762",
          "exception": false,
          "start_time": "2020-10-31T03:17:18.656204",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQiNcW75kxGF",
        "outputId": "bd5dd0c6-6bc6-4070-bec9-d872fc4dd8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0\n"
          ]
        }
      ],
      "source": [
        "!pip  install coloredlogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:17:46.284752Z",
          "iopub.status.busy": "2020-10-31T03:17:46.281554Z",
          "iopub.status.idle": "2020-10-31T03:17:46.301873Z",
          "shell.execute_reply": "2020-10-31T03:17:46.301346Z"
        },
        "papermill": {
          "duration": 0.049974,
          "end_time": "2020-10-31T03:17:46.302009",
          "exception": false,
          "start_time": "2020-10-31T03:17:46.252035",
          "status": "completed"
        },
        "tags": [],
        "id": "4PGk-dCdkxGF"
      },
      "outputs": [],
      "source": [
        "#wordtest.py\n",
        "\n",
        "import logging\n",
        "import coloredlogs\n",
        "import pickle\n",
        "\n",
        "logger = logging.getLogger('__file__')\n",
        "coloredlogs.install(level='INFO', logger=logger)\n",
        "\n",
        "def pickle_io(path, mode='r', obj=None):\n",
        "    \"\"\"\n",
        "    Convinient pickle load and dump.\n",
        "    \"\"\"\n",
        "    if mode in ['rb', 'r']:\n",
        "        logger.info(\"Loading obj from {}...\".format(path))\n",
        "        with open(path, 'rb') as f:\n",
        "            obj = pickle.load(f)\n",
        "        logger.info(\"Load obj successfully!\")\n",
        "        return obj\n",
        "    elif mode in ['wb', 'w']:\n",
        "        logger.info(\"Dumping obj to {}...\".format(path))\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(obj, f)\n",
        "        logger.info(\"Dump obj successfully!\")\n",
        "\n",
        "class WordTestResource(object):\n",
        "\n",
        "    def __init__(self, resource_path, verbose=False):\n",
        "\n",
        "        resource = pickle_io(resource_path, mode='r')\n",
        "\n",
        "        self.id2index = resource['id2index']\n",
        "        self.index2id = resource['index2id']\n",
        "        self.num_skills = len(self.id2index)\n",
        "\n",
        "        if verbose:\n",
        "            self.word2id = resource['word2id']\n",
        "            self.id2all = resource['id2all']\n",
        "            # rank0 already be set to a large number\n",
        "            self.words_by_rank = resource['words_by_rank']\n",
        "            self.pos2id = resource['pos2id']\n",
        "            self.words_by_rank.sort(key=lambda x: x[u'rank'])\n",
        "            self.id_by_rank = [x[u'word_id'] for x in self.words_by_rank]\n",
        "\n",
        "def str2bool(s):\n",
        "    if s not in {'False', 'True'}:\n",
        "        raise ValueError('Not a valid boolean string')\n",
        "    return s == 'True'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:17:46.338191Z",
          "iopub.status.busy": "2020-10-31T03:17:46.337294Z",
          "iopub.status.idle": "2020-10-31T03:17:46.340545Z",
          "shell.execute_reply": "2020-10-31T03:17:46.340012Z"
        },
        "papermill": {
          "duration": 0.023884,
          "end_time": "2020-10-31T03:17:46.340638",
          "exception": false,
          "start_time": "2020-10-31T03:17:46.316754",
          "status": "completed"
        },
        "tags": [],
        "id": "mBf_UKMQkxGF"
      },
      "outputs": [],
      "source": [
        "#config.py\n",
        "\n",
        "class DefaultConfig(object):\n",
        "    model = 'SAKT'\n",
        "    #train_data = \"../input/assist2015files/assist2015_train.csv\"  # train_data_path\n",
        "    #test_data = \"../input/assist2015files/assist2015_test.csv\"\n",
        "    batch_size = 4 #HDKIM 256\n",
        "    state_size = 200\n",
        "    num_heads = 5\n",
        "    max_len = 50\n",
        "    dropout = 0.1\n",
        "    max_epoch = 5 #10\n",
        "    lr = 3e-3\n",
        "    lr_decay = 0.9\n",
        "    max_grad_norm = 1.0\n",
        "    weight_decay = 0  # l2正则化因子\n",
        "\n",
        "    #JEREMY EDIT - adding parameters for typing tool\n",
        "    timestamp_buckets = 10 #for using timestamps as a difficulty measurement\n",
        "    lambda_time = 0.3 #for using time loss weight\n",
        "    word_padding = 0\n",
        "    log_time = True\n",
        "    #************************\n",
        "\n",
        "opt = DefaultConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:17:46.380079Z",
          "iopub.status.busy": "2020-10-31T03:17:46.379408Z",
          "iopub.status.idle": "2020-10-31T03:18:15.168133Z",
          "shell.execute_reply": "2020-10-31T03:18:15.167178Z"
        },
        "papermill": {
          "duration": 28.812971,
          "end_time": "2020-10-31T03:18:15.168261",
          "exception": false,
          "start_time": "2020-10-31T03:17:46.355290",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieviGfYXkxGG",
        "outputId": "b0e09c52-072a-4799-ab63-3da1dad0d7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting prefetch_generator\n",
            "  Downloading prefetch_generator-1.0.3.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: prefetch_generator\n",
            "  Building wheel for prefetch_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prefetch_generator: filename=prefetch_generator-1.0.3-py3-none-any.whl size=4758 sha256=897efbb967914cc634222efd38a5d7f04cf34e86f7602c29965de8b6fc8ad67c\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/88/c7/3b5afc342fc80a599ce41ba9000cf8a71261991c35cf088edf\n",
            "Successfully built prefetch_generator\n",
            "Installing collected packages: prefetch_generator\n",
            "Successfully installed prefetch_generator-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install prefetch_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2020-10-31T03:18:15.228207Z",
          "iopub.status.busy": "2020-10-31T03:18:15.222749Z",
          "iopub.status.idle": "2020-10-31T03:18:15.326295Z",
          "shell.execute_reply": "2020-10-31T03:18:15.325298Z"
        },
        "papermill": {
          "duration": 0.139551,
          "end_time": "2020-10-31T03:18:15.326407",
          "exception": false,
          "start_time": "2020-10-31T03:18:15.186856",
          "status": "completed"
        },
        "tags": [],
        "id": "qj0FwizukxGG"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "#from config import DefaultConfig\n",
        "#from wordtest import WordTestResource\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from prefetch_generator import BackgroundGenerator\n",
        "\n",
        "import joblib #HDKIM\n",
        "\n",
        "class Data(Dataset):\n",
        "    #HDKIM def __init__(self, train=True):\n",
        "    def __init__(self, df, train=True):\n",
        "        start_time = time.time()\n",
        "        #HDKIM if train:\n",
        "        #HDKIM    fileName = opt.train_data\n",
        "        #HDKIM else:\n",
        "        #HDKIM     fileName = opt.test_data\n",
        "        self.students = []\n",
        "        self.max_skill_num = 0\n",
        "        begin_index = 1e9\n",
        "\n",
        "        #HDKIM with open(fileName, \"r\") as csvfile:\n",
        "            #HDKIM for num_ques, ques, ans in itertools.zip_longest(*[csvfile] * 3):\n",
        "                #HDKIM num_ques = int(num_ques.strip().strip(','))\n",
        "                #HDKIM ques = [int(q) for q in ques.strip().strip(',').split(',')]\n",
        "                #HDKIM ans = [int(a) for a in ans.strip().strip(',').split(',')]\n",
        "        for index, row in df.iterrows():\n",
        "                num_ques = int(row['num_ques'])\n",
        "                #print(row['num_ques'])\n",
        "                #print(row['ques'])\n",
        "                #print(row['ans'])\n",
        "                ques = [int(q) for q in row['ques']]\n",
        "                ans = [int(a) for a in row['ans']]\n",
        "\n",
        "                #JEREMY EDIT - adding times to also be returned as a tensor for SAKT\n",
        "                times = [float(t) for t in row['times']]\n",
        "                #************************\n",
        "\n",
        "                tmp_max_skill = max(ques)\n",
        "                tmp_min_skill = min(ques)\n",
        "                begin_index = min(tmp_min_skill, begin_index)\n",
        "                self.max_skill_num = max(tmp_max_skill, self.max_skill_num)\n",
        "\n",
        "                #HDKIM if (num_ques <= 2):\n",
        "                #HDKIM     continue\n",
        "                #HDKIM elif num_ques <= opt.max_len:\n",
        "                #HDKIM if num_ques <= opt.max_len:\n",
        "                '''\n",
        "                if num_ques <= opt.max_len:\n",
        "                    problems = np.zeros(opt.max_len, dtype=np.int64)\n",
        "                    correct = np.ones(opt.max_len, dtype=np.int64)\n",
        "                    problems[-num_ques:] = ques[-num_ques:]\n",
        "                    correct[-num_ques:] = ans[-num_ques:]\n",
        "                    self.students.append((num_ques, problems, correct))\n",
        "                else:\n",
        "                    start_idx = 0\n",
        "                    while opt.max_len + start_idx <= num_ques:\n",
        "                        problems = np.array(ques[start_idx:opt.max_len + start_idx])\n",
        "                        correct = np.array(ans[start_idx:opt.max_len + start_idx])\n",
        "                        tup = (opt.max_len, problems, correct)\n",
        "                        start_idx += opt.max_len\n",
        "                        self.students.append(tup)\n",
        "                    left_num_ques = num_ques - start_idx\n",
        "                '''\n",
        "                #HDKIM\n",
        "                # first part of the student\n",
        "                copy_len = opt.max_len - 1\n",
        "                if copy_len > num_ques:\n",
        "                    copy_len = num_ques\n",
        "                problems = np.zeros(opt.max_len, dtype=np.int64)\n",
        "                correct = np.ones(opt.max_len, dtype=np.int64)\n",
        "\n",
        "                #JEREMY - adding time\n",
        "                time_array = np.zeros(opt.max_len, dtype=np.float32)\n",
        "\n",
        "                problems[-copy_len:] = ques[-copy_len:]\n",
        "                correct[-copy_len:] = ans[-copy_len:]\n",
        "                time_array[-copy_len:] = np.array(times[-copy_len:], dtype=np.float32)\n",
        "\n",
        "                tup = (copy_len, problems, correct, time_array)\n",
        "                #****************\n",
        "\n",
        "                self.students.append(tup)\n",
        "\n",
        "                if num_ques > opt.max_len - 1:\n",
        "                    start_idx = opt.max_len - 1\n",
        "                    while opt.max_len - 1 + start_idx <= num_ques:\n",
        "                        problems = np.array(ques[(start_idx-1):(start_idx + opt.max_len -1 )])\n",
        "                        correct = np.array(ans[(start_idx-1):(start_idx + opt.max_len -1)])\n",
        "\n",
        "                        #JEREMY TIME ADDED\n",
        "                        time_array = np.array(times[(start_idx-1):(start_idx + opt.max_len -1)])\n",
        "\n",
        "                        tup = (opt.max_len, problems, correct, time_array)\n",
        "                        #***\n",
        "\n",
        "                        self.students.append(tup)\n",
        "                        start_idx += (opt.max_len-1)\n",
        "                    left_num_ques = num_ques - start_idx\n",
        "\n",
        "                    #HDKIM if left_num_ques>2:\n",
        "                    if left_num_ques>0:\n",
        "                        problems = np.zeros(opt.max_len, dtype=np.int64)\n",
        "                        correct = np.ones(opt.max_len, dtype=np.int64)\n",
        "\n",
        "                        #JEREMY - adding time\n",
        "                        time_array = np.zeros(opt.max_len, dtype=np.float32)\n",
        "                        time_array[-left_num_ques:] = times[-left_num_ques:]\n",
        "                        #****\n",
        "\n",
        "                        problems[-left_num_ques:] = ques[-left_num_ques:]\n",
        "                        correct[-left_num_ques:] = ans[-left_num_ques:]\n",
        "\n",
        "                        #added time to tup\n",
        "                        tup = (left_num_ques, problems, correct, time_array)\n",
        "\n",
        "                        self.students.append(tup)\n",
        "\n",
        "        if train==False:\n",
        "            if len(self.students) % opt.batch_size > 0:\n",
        "                for i in range(opt.batch_size - (len(self.students) % opt.batch_size)):\n",
        "                    self.students.append(tup)\n",
        "\n",
        "        print(len(self.students))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        student = self.students[index]\n",
        "        problems = student[1]\n",
        "        #print(\"before\",problems)\n",
        "        correct = student[2]\n",
        "\n",
        "        #Jeremy added to SAKT and replaced ''' code\n",
        "        times = student[3]\n",
        "\n",
        "        '''\n",
        "        #HDKIM x = np.zeros(opt.max_len - 1)\n",
        "        x = problems[:-1].copy()\n",
        "        # we assume max_skill_num + 1 = num_skills because skill index starts from 0 to max_skill_num\n",
        "        x += (correct[:-1] == 1) * (self.max_skill_num + 1)\n",
        "        problems = problems[1:]\n",
        "        correct = correct[1:]\n",
        "\n",
        "        #print(\"after\",problems)\n",
        "\n",
        "        return x, problems, correct\n",
        "        '''\n",
        "\n",
        "        question_in = problems[:-1].copy()\n",
        "        answer_in = correct[:-1].copy()\n",
        "        time_in = times[:-1].copy()\n",
        "\n",
        "        example_bins = np.array([150,250,350,500,700,900,1200,1600,2200], dtype=np.float32)\n",
        "        timestamp_bucket_in = np.digitize(time_in, example_bins).astype(np.int64)\n",
        "\n",
        "        question_next = problems[1:].copy()\n",
        "        correct_target = correct[1:].copy()\n",
        "        time_target = times[1:].copy()\n",
        "\n",
        "        return question_in, answer_in, timestamp_bucket_in, question_next, correct_target, time_target\n",
        "        #*********\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.students)\n",
        "\n",
        "\n",
        "\n",
        "class DataLoaderX(DataLoader):\n",
        "\n",
        "    def __iter__(self):\n",
        "        return BackgroundGenerator(super().__iter__())\n",
        "\n",
        "\n",
        "class DataPrefetcher():\n",
        "    def __init__(self, loader, device):\n",
        "        self.loader = iter(loader)\n",
        "\n",
        "        self.device = torch.device(device) if isinstance(device, str) else device\n",
        "\n",
        "        if self.device.type == \"cuda\":\n",
        "            self.stream = torch.cuda.Stream()\n",
        "        else:\n",
        "            self.stream = None\n",
        "\n",
        "        # With Amp, it isn't necessary to manually convert data to half.\n",
        "        # if args.fp16:\n",
        "        #     self.mean = self.mean.half()\n",
        "        #     self.std = self.std.half()\n",
        "        self.preload()\n",
        "\n",
        "    def preload(self):\n",
        "        try:\n",
        "            self.batch = next(self.loader)\n",
        "        except StopIteration:\n",
        "            self.batch = None\n",
        "            return\n",
        "\n",
        "        #using CUDA\n",
        "        if self.stream is not None:\n",
        "          with torch.cuda.stream(self.stream):\n",
        "              for k in range(len(self.batch)):\n",
        "                  self.batch[k] = self.batch[k].to(device=self.device, non_blocking=True)\n",
        "        #using CPU\n",
        "        else:\n",
        "            for k in range(len(self.batch)):\n",
        "                self.batch[k] = self.batch[k].to(device=self.device)\n",
        "\n",
        "            # With Amp, it isn't necessary to manually convert data to half.\n",
        "            # if args.fp16:\n",
        "            #     self.next_input = self.next_input.half()\n",
        "            # else:\n",
        "            #     self.next_input = self.next_input.float()\n",
        "\n",
        "    def next(self):\n",
        "      if self.stream is not None:\n",
        "          torch.cuda.current_stream().wait_stream(self.stream)\n",
        "\n",
        "      batch = self.batch\n",
        "      self.preload()\n",
        "      return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:18:15.365373Z",
          "iopub.status.busy": "2020-10-31T03:18:15.364478Z",
          "iopub.status.idle": "2020-10-31T03:18:15.392574Z",
          "shell.execute_reply": "2020-10-31T03:18:15.392074Z"
        },
        "papermill": {
          "duration": 0.048568,
          "end_time": "2020-10-31T03:18:15.392668",
          "exception": false,
          "start_time": "2020-10-31T03:18:15.344100",
          "status": "completed"
        },
        "tags": [],
        "id": "1mfd7enlkxGH"
      },
      "outputs": [],
      "source": [
        "# student_model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "#from config import DefaultConfig\n",
        "#from utils import subsequent_mask\n",
        "from torch.autograd import Variable\n",
        "#from multihead_attn import MultiHeadedAttention\n",
        "from torch.nn import LayerNorm\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, state_size, dropout=0.1, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        self.pe = torch.zeros(max_len, state_size)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0.0, state_size, 2) *\n",
        "                             -(math.log(10000.0) / state_size))\n",
        "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = self.pe.unsqueeze(0)\n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)],\n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class student_model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_skills, state_size, num_heads=2, dropout=0.2, infer=False):\n",
        "        super(student_model, self).__init__()\n",
        "        self.infer = infer\n",
        "        self.num_skills = num_skills\n",
        "        self.state_size = state_size\n",
        "\n",
        "        '''JEREMY COMMENTED OUT\n",
        "        # we use the (num_skills * 2 + 1) as key padding_index\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_skills*2+1,\n",
        "                                      embedding_dim=state_size)\n",
        "                                      # padding_idx=num_skills*2\n",
        "        '''\n",
        "        # self.position_embedding = PositionalEncoding(state_size)\n",
        "        self.position_embedding = nn.Embedding(num_embeddings=opt.max_len-1,\n",
        "                                               embedding_dim=state_size)\n",
        "        '''JEREMY COMMENTED OUT\n",
        "        # we use the (num_skills + 1) as query padding_index\n",
        "        self.problem_embedding = nn.Embedding(num_embeddings=num_skills+1,\n",
        "                                      embedding_dim=state_size)\n",
        "                                      # padding_idx=num_skills)\n",
        "        '''\n",
        "\n",
        "        self.multi_attn = MultiHeadedAttention(h=num_heads, d_model=state_size, dropout=dropout, infer=self.infer)\n",
        "        self.feedforward1 = nn.Linear(in_features=state_size, out_features=state_size)\n",
        "        self.feedforward2 = nn.Linear(in_features=state_size, out_features=state_size)\n",
        "\n",
        "        '''JEREMY COMMENTED OUT\n",
        "        self.pred_layer = nn.Linear(in_features=state_size, out_features=num_skills)\n",
        "        '''\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layernorm = LayerNorm(state_size)\n",
        "\n",
        "        #JEREMY IMPLEMENTING\n",
        "        self.word_emb = nn.Embedding(num_skills+1, state_size) #padding is +1\n",
        "        self.resp_emb = nn.Embedding(2, state_size) #how correct, 0 or 1\n",
        "        self.time_emb = nn.Embedding(opt.timestamp_buckets, state_size)\n",
        "\n",
        "        #2 error heads\n",
        "        self.error_head = nn.Linear(state_size, 1) #correctness logits\n",
        "        self.time_head = nn.Linear(state_size, 1) #log time prediction\n",
        "        #****************\n",
        "\n",
        "    #JEREMY - making forward predict 2 sequences (error and time)\n",
        "    '''\n",
        "    def forward(self, x, problems, target_index):\n",
        "        # self.key_masks = torch.unsqueeze( (x!=self.num_skills*2).int(), -1)\n",
        "        # self.problem_masks = torch.unsqueeze( (problems!=self.num_skills).int(), -1)\n",
        "        x = self.embedding(x)\n",
        "        pe = self.position_embedding(torch.arange(x.size(1)).unsqueeze(0).cuda())\n",
        "        x += pe\n",
        "        # x = self.position_embedding(x)\n",
        "        problems = self.problem_embedding(problems)\n",
        "        # self.key_masks = self.key_masks.type_as(x)\n",
        "        # self.problem_masks = self.problem_masks.type_as(problems)\n",
        "        # x *= self.key_masks\n",
        "        # problems *= self.problem_masks\n",
        "        x = self.dropout(x)\n",
        "        res = self.multi_attn(query=self.layernorm(problems), key=x, value=x,\n",
        "                              key_masks=None, query_masks=None, future_masks=None)\n",
        "        outputs = F.relu(self.feedforward1(res))\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.dropout(self.feedforward2(outputs))\n",
        "        # Residual connection\n",
        "        outputs += self.layernorm(res)\n",
        "        outputs = self.layernorm(outputs)\n",
        "        logits = self.pred_layer(outputs)\n",
        "\n",
        "        #HDKIM logits = logits.contiguous().view(logits.size(0) * opt.max_len - 1, -1)\n",
        "        logits = logits.contiguous().view(logits.size(0) * (opt.max_len - 1), -1)\n",
        "        logits = logits.contiguous().view(-1)\n",
        "        selected_logits = torch.gather(logits, 0, torch.LongTensor(target_index).cuda())\n",
        "        return selected_logits\n",
        "        '''\n",
        "    def forward(self, ques_in, ans_in, timebucket, next_ques):\n",
        "        input_x = self.word_emb(ques_in) + self.resp_emb(ans_in) + self.time_emb(timebucket)\n",
        "\n",
        "        query = self.word_emb(next_ques)#getting next words\n",
        "\n",
        "        #positional embedding implementation\n",
        "        pe = self.position_embedding(torch.arange(input_x.size(1)).unsqueeze(0).to(input_x.device))\n",
        "        input_x += pe\n",
        "        query += pe\n",
        "\n",
        "        input_x = self.dropout(input_x)\n",
        "        res = self.multi_attn(query=self.layernorm(query), key=input_x, value=input_x,\n",
        "                              key_masks=None, query_masks=None, future_masks=None)\n",
        "\n",
        "        out = F.relu(self.feedforward1(res))\n",
        "        out = self.dropout(out)\n",
        "        out = self.dropout(self.feedforward2(out))\n",
        "        out += self.layernorm(res)\n",
        "        out = self.layernorm(out)\n",
        "\n",
        "        error_logits = self.error_head(out).squeeze(-1)\n",
        "        predicted_times = self.time_head(out).squeeze(-1)\n",
        "\n",
        "        return error_logits, predicted_times\n",
        "    #**************************************************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:18:15.452497Z",
          "iopub.status.busy": "2020-10-31T03:18:15.436776Z",
          "iopub.status.idle": "2020-10-31T03:18:16.396636Z",
          "shell.execute_reply": "2020-10-31T03:18:16.395081Z"
        },
        "papermill": {
          "duration": 0.98639,
          "end_time": "2020-10-31T03:18:16.396780",
          "exception": false,
          "start_time": "2020-10-31T03:18:15.410390",
          "status": "completed"
        },
        "tags": [],
        "id": "BTfK0ayAkxGH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#JEREMY Changed the whole function (params and how it works)\n",
        "\n",
        "def run_epoch(m, dataloader, optimizer, scheduler,\n",
        "              criterion1, criterion2,\n",
        "              pad_id, lambda_time,\n",
        "              epoch_id=None, writer=None, is_training=True):\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "    m.to(device)\n",
        "    if is_training:\n",
        "        m.train()\n",
        "    else:\n",
        "        m.eval()\n",
        "\n",
        "    actual_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    #using regression in log space for normalizing times\n",
        "    actual_log_times = []\n",
        "    pred_log_times = []\n",
        "\n",
        "    num_batch = len(dataloader)\n",
        "    prefetcher = DataPrefetcher(dataloader, device=device)\n",
        "    batch = prefetcher.next()\n",
        "    counter = 0\n",
        "\n",
        "    if is_training:\n",
        "        while batch is not None:\n",
        "            #new batches with added params\n",
        "            q_in, resp_in, time_bucket_in, q_next, y_correct, y_time = batch\n",
        "\n",
        "            q_in = q_in.long().to(device)\n",
        "            resp_in = resp_in.long().to(device)\n",
        "            time_bucket_in = time_bucket_in.long().to(device)\n",
        "            q_next = q_next.long().to(device)\n",
        "\n",
        "            y_correct = y_correct.float().to(device) #ensures either 0 or 1\n",
        "            y_time = y_time.float().to(device)\n",
        "\n",
        "            #ensures padding is ignored\n",
        "            mask = (q_next != pad_id)\n",
        "            if mask.sum().item() == 0:\n",
        "                batch = prefetcher.next()\n",
        "                continue\n",
        "\n",
        "            #applies log to time targets\n",
        "            y_log_time = torch.log1p(y_time)\n",
        "\n",
        "            #forward pass\n",
        "            err_logits, pred_log_time = m(q_in, resp_in, time_bucket_in, q_next)\n",
        "\n",
        "            #get losses for criterions\n",
        "            loss_err = criterion1(err_logits[mask], y_correct[mask])\n",
        "            loss_time = criterion2(pred_log_time[mask], y_log_time[mask])\n",
        "\n",
        "            loss = loss_err + lambda_time * loss_time\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(m.parameters(), opt.max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            #measures correctness of SAKT\n",
        "            with torch.no_grad():\n",
        "                pred_prob = torch.sigmoid(err_logits)\n",
        "                actual_labels += list(y_correct[mask].detach().cpu().numpy())\n",
        "                pred_labels += list(pred_prob[mask].detach().cpu().numpy())\n",
        "\n",
        "                actual_log_times += list(y_log_time[mask].detach().cpu().numpy())\n",
        "                pred_log_times += list(pred_log_time[mask].detach().cpu().numpy())\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "            if counter % 500 == 0:\n",
        "                print(f\"\\r batch{counter}/{num_batch}\", end=\"\")\n",
        "\n",
        "            if counter >= num_batch:\n",
        "                break\n",
        "\n",
        "            batch = prefetcher.next()\n",
        "\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            while batch is not None:\n",
        "                q_in, resp_in, time_bucket_in, q_next, y_correct, y_time = batch\n",
        "\n",
        "                q_in = q_in.long().to(device)\n",
        "                resp_in = resp_in.long().to(device)\n",
        "                time_bucket_in = time_bucket_in.long().to(device)\n",
        "                q_next = q_next.long().to(device)\n",
        "\n",
        "                y_correct = y_correct.float().to(device)\n",
        "                y_time = y_time.float().to(device)\n",
        "\n",
        "                mask = (q_next != pad_id)\n",
        "                if mask.sum().item() == 0:\n",
        "                    batch = prefetcher.next()\n",
        "                    continue\n",
        "\n",
        "                y_log_time = torch.log1p(y_time)\n",
        "\n",
        "                err_logits, pred_log_time = m(q_in, resp_in, time_bucket_in, q_next)\n",
        "                pred_prob = torch.sigmoid(err_logits)\n",
        "\n",
        "                actual_labels += list(y_correct[mask].cpu().numpy())\n",
        "                pred_labels += list(pred_prob[mask].cpu().numpy())\n",
        "\n",
        "                actual_log_times += list(y_log_time[mask].cpu().numpy())\n",
        "                pred_log_times += list(pred_log_time[mask].cpu().numpy())\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "                if counter % 500 == 0:\n",
        "                    print(f\"\\r batch{counter}/{num_batch}\", end=\"\")\n",
        "\n",
        "                if counter >= num_batch:\n",
        "                    break\n",
        "\n",
        "                batch = prefetcher.next()\n",
        "\n",
        "    #getting the SAKT scores\n",
        "    rmse = sqrt(mean_squared_error(actual_labels, pred_labels))#root mean square error can be used for evaluation\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(actual_labels, pred_labels, pos_label=1)\n",
        "    auc = metrics.auc(fpr, tpr)#represents degree of seperation between true positive rate and false positive rate\n",
        "    r2 = r2_score(actual_labels, pred_labels)#gets \"goodness of fit\" / coefficient of determination\n",
        "    accuracy = metrics.accuracy_score(actual_labels, np.array(pred_labels) >= 0.5)\n",
        "\n",
        "    #gets the root mean square error for log times\n",
        "    time_rmse = sqrt(mean_squared_error(actual_log_times, pred_log_times)) if len(actual_log_times) else None\n",
        "\n",
        "    return rmse, auc, r2, accuracy, pred_labels, time_rmse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIljsZRQmP8i",
        "outputId": "ca8a36a8-43b8-4d41-ab50-547b1ae0279a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/\"Colab Notebooks\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIP1Q8u5mqtY",
        "outputId": "beb4490a-af03-4980-cc0d-99dce50947df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Emoji Test.ipynb'   Lab3.ipynb\t\t\t\t       Untitled\n",
            " images\t\t     SAKT_Model_Training_and_Exporting.ipynb   Untitled0.ipynb\n",
            " images_faces_only   typing_log.csv\t\t\t       Untitled1.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"torch cuda:\", torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmkRMf6gn_1a",
        "outputId": "a6515372-8169-436a-de0b-a6026cf87143"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available: True\n",
            "torch cuda: 12.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-31T03:18:16.459394Z",
          "iopub.status.busy": "2020-10-31T03:18:16.456464Z",
          "iopub.status.idle": "2020-10-31T03:18:23.984395Z",
          "shell.execute_reply": "2020-10-31T03:18:23.985078Z"
        },
        "papermill": {
          "duration": 7.569982,
          "end_time": "2020-10-31T03:18:23.985255",
          "exception": false,
          "start_time": "2020-10-31T03:18:16.415273",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "2cVEy0PKkxGH",
        "outputId": "8a263306-c348-4f9e-ef01-87c138726a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "4\n",
            "\n",
            "current epoch: 1 out of 5\n",
            "valid AUC = 0.4720 ACC = 0.8333 time_rmse(log) = 4.427538467240657\n",
            "best bundle saved in: ./artifacts/sakt_typing_bundle.pt\n",
            "\n",
            "current epoch: 2 out of 5\n",
            "valid AUC = 0.3440 ACC = 0.8333 time_rmse(log) = 2.0200410201575627\n",
            "\n",
            "current epoch: 3 out of 5\n",
            "valid AUC = 0.3040 ACC = 0.8333 time_rmse(log) = 0.7821954304738177\n",
            "\n",
            "current epoch: 4 out of 5\n",
            "valid AUC = 0.3120 ACC = 0.8333 time_rmse(log) = 0.6355326735848387\n",
            "\n",
            "current epoch: 5 out of 5\n",
            "valid AUC = 0.3360 ACC = 0.8333 time_rmse(log) = 0.6859392291115285\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1433afd7-0424-4944-b5e6-d920cd42327b\", \"sakt_typing_bundle.pt\", 1097893)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAKT training finished\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#JEREMY changed whole function and how it works, and so it would work in our coding environment\n",
        "\n",
        "# assumes these already exist from your notebook:\n",
        "# opt, Data, DataLoaderX, student_model, run_epoch\n",
        "\n",
        "def build_word_vocab(typing_log_df, min_freq=1):\n",
        "    #words returned from typing logs\n",
        "\n",
        "    counts = typing_log_df[\"word\"].value_counts()\n",
        "    words = counts[counts >= min_freq].index.tolist()\n",
        "    word2id = {w: (i + 1) for i, w in enumerate(words)}  # start at 1\n",
        "    return word2id\n",
        "\n",
        "def make_sequences_df(typing_log_df, word2id, max_seq_len):\n",
        "    #turns typing log into num_ques,ques,ans,times with one user per row\n",
        "\n",
        "    rows = []\n",
        "    for user_id, g in typing_log_df.groupby(\"user_id\"):\n",
        "        #figures out timestamp order (order times are inputted)\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "\n",
        "        ques = [word2id.get(w, 0) for w in g[\"word\"].astype(str).tolist()]\n",
        "        ans  = [1 if m == 0 else 0 for m in g[\"mistypes\"].astype(int).tolist()]\n",
        "        times = g[\"time_ms\"].astype(float).tolist()\n",
        "\n",
        "        #using chunks\n",
        "        for start in range(0, len(ques), max_seq_len):\n",
        "            q_chunk = ques[start:start+max_seq_len]\n",
        "            a_chunk = ans[start:start+max_seq_len]\n",
        "            t_chunk = times[start:start+max_seq_len]\n",
        "            if len(q_chunk) < 2:\n",
        "                continue  #needs 2 steps to properly predict\n",
        "\n",
        "            rows.append({\n",
        "                \"num_ques\": len(q_chunk),\n",
        "                \"ques\": q_chunk,\n",
        "                \"ans\": a_chunk,\n",
        "                \"times\": t_chunk\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def split_train_valid(seqs_df, valid_frac=0.2, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(seqs_df))\n",
        "    rng.shuffle(idx)\n",
        "\n",
        "    if len(idx) < 2:\n",
        "        return seqs_df.copy(), seqs_df.iloc[:0].copy() #if there's not enough data to split\n",
        "\n",
        "    n_valid = max(1, int(len(idx) * valid_frac))\n",
        "    n_valid = min(n_valid, len(idx) - 1)\n",
        "    #above ensures minimum of 1 train sample\n",
        "\n",
        "    valid_idx = idx[:n_valid]\n",
        "    train_idx = idx[n_valid:]\n",
        "\n",
        "    train_df = seqs_df.iloc[train_idx].reset_index(drop=True)\n",
        "    valid_df = seqs_df.iloc[valid_idx].reset_index(drop=True)\n",
        "    return train_df, valid_df\n",
        "\n",
        "def save_sakt_bundle(save_path, model, opt, word2id, time_bins_ms):\n",
        "    bundle = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"opt\": {k: getattr(opt, k) for k in dir(opt)\n",
        "                if not k.startswith(\"__\") and not callable(getattr(opt, k))},\n",
        "        \"word2id\": word2id,\n",
        "        \"id2word\": {str(v): k for k, v in word2id.items()},\n",
        "        \"time_bins_ms\": list(map(float, time_bins_ms)),\n",
        "    }\n",
        "    torch.save(bundle, save_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #load typing log\n",
        "    typing_log = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/typing_log.csv\")\n",
        "\n",
        "    #build vocab\n",
        "    word2id = build_word_vocab(typing_log, min_freq=1)\n",
        "\n",
        "    #for SAKT how may skills (in this case words)\n",
        "    opt.num_skills = max(word2id.values()) + 1\n",
        "\n",
        "    seqs_df = make_sequences_df(typing_log, word2id, 30) #max seq will always be 100 for now (not too long not too short) #changed to 30 due to less typing logs\n",
        "    train_df, valid_df = split_train_valid(seqs_df, valid_frac=0.2, seed=0)\n",
        "\n",
        "    time_bins_ms = np.array(np.linspace(800, 18000, 10), dtype=np.float32)\n",
        "\n",
        "    #train, validate, and export model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_dataset = Data(train_df, train=True)\n",
        "    valid_dataset = Data(valid_df, train=False)\n",
        "\n",
        "    train_loader = DataLoaderX(train_dataset, batch_size=opt.batch_size, num_workers=2,\n",
        "                               pin_memory=True, shuffle=True)\n",
        "    valid_loader = DataLoaderX(valid_dataset, batch_size=opt.batch_size, num_workers=2,\n",
        "                               pin_memory=True, shuffle=False)\n",
        "\n",
        "    model = student_model(\n",
        "        num_skills=opt.num_skills,\n",
        "        state_size=opt.state_size,\n",
        "        num_heads=opt.num_heads,\n",
        "        dropout=opt.dropout,\n",
        "        infer=False\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=opt.lr_decay)\n",
        "\n",
        "    criterion_err = nn.BCEWithLogitsLoss()\n",
        "    criterion_time = nn.SmoothL1Loss()\n",
        "\n",
        "    #trained SAKT model we can use:\n",
        "    os.makedirs(\"./artifacts\", exist_ok=True)\n",
        "    save_path = \"./artifacts/sakt_typing_bundle.pt\"\n",
        "\n",
        "    best_auc = -1.0\n",
        "\n",
        "    for epoch in range(opt.max_epoch):\n",
        "        print(f\"\\ncurrent epoch: {epoch+1} out of {opt.max_epoch}\")\n",
        "\n",
        "        #train model\n",
        "        train_rmse, train_auc, train_r2, train_acc, _, train_time_rmse = run_epoch(\n",
        "            model, train_loader,\n",
        "            optimizer, scheduler,\n",
        "            criterion_err, criterion_time,\n",
        "            pad_id=opt.word_padding,\n",
        "            lambda_time=opt.lambda_time,\n",
        "            is_training=True\n",
        "        )\n",
        "\n",
        "        #checking if any valid samples\n",
        "        if len(valid_df) == 0:\n",
        "          print(\"skipping validation - no valid samples\")\n",
        "          continue\n",
        "\n",
        "        #validate model\n",
        "        with torch.no_grad():\n",
        "            val_rmse, val_auc, val_r2, val_acc, _, val_time_rmse = run_epoch(\n",
        "                model, valid_loader,\n",
        "                optimizer=None, scheduler=None,\n",
        "                criterion1=criterion_err, criterion2=criterion_time,\n",
        "                pad_id=opt.word_padding,\n",
        "                lambda_time=opt.lambda_time,\n",
        "                is_training=False\n",
        "            )\n",
        "\n",
        "        print(f\"valid AUC = {val_auc:.4f} ACC = {val_acc:.4f} time_rmse(log) = {val_time_rmse}\")\n",
        "\n",
        "        #save best model\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            save_sakt_bundle(save_path, model, opt, word2id, time_bins_ms)\n",
        "            print(f\"best bundle saved in: {save_path}\")\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(\"./artifacts/sakt_typing_bundle.pt\")\n",
        "\n",
        "    print(\"SAKT training finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "papermill": {
          "duration": 0.022364,
          "end_time": "2020-10-31T03:18:24.030775",
          "exception": false,
          "start_time": "2020-10-31T03:18:24.008411",
          "status": "completed"
        },
        "tags": [],
        "id": "oC_6L3qWkxGH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 102.671913,
      "end_time": "2020-10-31T03:18:24.561007",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-10-31T03:16:41.889094",
      "version": "2.1.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}